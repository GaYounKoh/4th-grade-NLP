{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network: 순환신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW: Continuous Back Of Words\n",
    "- 조건부 확률 <br>\n",
    "\n",
    "context 먼저 뽑았을 때 그 다음에 label 뽑게 될 확률\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터로 단어를 얼마나 쉽게 만드ㄴ는지  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화함수로 sigmoid아닌 하이퍼볼릭탄젠트 사용. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 삼각함수\n",
    "\n",
    "하이퍼볼릭 사인함스, 하이퍼볼릵 코사인, 하이퍼볼릭 탄젠트... <br>\n",
    "계산 <br>\n",
    "도함수 유도하기... <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치미분 안쓰고 역전파 쓰는 이유: 계산비용 대폭 절감위해 <br>\n",
    "컴퓨터가 빠르게 계산할 수 있는게 덧셈&곱셈 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. 왜 하이퍼볼릭 탄젠트 앞에 싸인 코싸인 탄젠트 이런게 붙냐?<br>\n",
    "Q2. 왜 하이퍼볼릭(쌍곡)이냐? <br>\n",
    "\n",
    "A2. 식 (cosh)^2 - (sinh)^2 = 1 때문 <br>\n",
    "\n",
    "- 원 (cos세타)^2 + (sin세타)^2 = 1 위의 한 점\n",
    "> (cos세타, sin세타)\n",
    "\n",
    "- 타원 (x/a)^2 + (y/b)^2 = 1 위의 한 점\n",
    "> (a\\*cos세타 b*cos세타)\n",
    "\n",
    "- 쌍곡선 (x/a)^2 - (y/2)^2 = 1 위의 한 점\n",
    "> (a\\*cosh세타 b*cosh세타) <br>\n",
    "> (cosh)^2 - (sinh)^2 = 1 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아다마르 프로덕트: 각 원소 별로 곱하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN class (모듈..?)\n",
    "* ./common/time_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.np import *  # import numpy as np (or import cupy as np)\n",
    "from common.layers import *\n",
    "from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    # 역전파 구현 코드 (진짜 그냥 수식을 그대로 구현한.)\n",
    "    def backward(self, dh_next): # 역전파 하려면 흘러들어온 미분 필요. dh_next: 입력인수, 흘러들어온 미분.\n",
    "                                                                # 손실함수를 h_next로 미분한 gradient일 것.\n",
    "        Wx, Wh, b = self.params # 위에서 만든 리스트\n",
    "                                 # Wx: 데이터 x의 weight matrix\n",
    "                                 # Wh: 전 시각에서 흘러들어온 hidden_state에 대한 weight matrix\n",
    "                                 # b: 편향벡터\n",
    "        x, h_prev, h_next = self.cache # 순전파(forward) 할 때 만들어둔 튜플\n",
    "                                 # x: input data\n",
    "                                 # h_prev: 전 시각에서 흘러들어온 hidden_state\n",
    "                                 # h_next: 다음 시각으로 보내는 hidden_state\n",
    "        dt = dh_next * (1 - h_next ** 2) # 흘러 들어온 미분이 하이퍼볼릭탄젠트 함수를 지남.\n",
    "                                          # 하이퍼볼릭탄젠트 미분 = (1-y^2);    y=h_next\n",
    "                                          # 흘러들어온 미분(dh_next)과 함께 곱히기(아다마르 프로덕트)\n",
    "        db = np.sum(dt, axis=0) # 하이퍼 볼릭 탄젠트를 지난 미분 dh가 덧셈노드라서 그대로 내려갈텐데,\n",
    "                                # b: 편향벡터,\n",
    "                                    # 만약 데이터가 배치처리를 해서 여러 장일 경우에는 편향벡터를 각 행에 그대로 카피해서 집어넣음.\n",
    "                                    # 들어온 미분행렬에 대해 각 행에 대해서 카피해서 집어 넣는다 = repeat node 지나간다는 얘기\n",
    "                                    # repeat node의 역전파 = sum node\n",
    "\n",
    "                                # 들어오는 미분 행렬을 각 열에 대해 더해주기 == sum node\n",
    "                                # b가 벡터이면 손실함수를 b로 미분한 gradient 역시 벡터가 돼야 함.\n",
    "                                # 들어온 미분 행렬을 각 열에 대해 더해줘야 벡터가 됨\n",
    "                                    # 즉, 각 열에 대해 흘러 들어온 미분을 다 더하란 소리\n",
    "        dWh = np.dot(h_prev.T, dt) # 흘러들어온 미분(dt)은 덧셈노드를 지나가니까 그대로 dt\n",
    "                                   # 거기에 엇갈려서 h_Transpose한 것을 곱하라는 것\n",
    "        dh_prev = np.dot(dt, Wh.T) # 흘러들어온 미분 dt에 엇갈려서 Wh_Transpose해서 곱하라는 것\n",
    "        dWx = np.dot(x.T, dt) # 흘러들어온 미분 dt에 엇갈려서 input data x를 x_Transpose해서 곱하라는 것\n",
    "        dx = np.dot(dt, Wx.T) # 흘러들어온 미분 dt에 엇갈려서 Wx_Transpose해서 곱하라는 것\n",
    "\n",
    "        # zero행렬들의 모음이었던 grads 역전파의 결과들로 채우기 (update)\n",
    "        self.grads[0][...] = dWx # Wx에 대한 gradient\n",
    "        self.grads[1][...] = dWh # Wh에 대한 gradient\n",
    "        self.grads[2][...] = db # 편향벡터 b에 대한 gradient\n",
    "        #### 이 층에 대한 학습은 gradient로 하게 됨.\n",
    "\n",
    "        return dx, dh_prev # 데이터에 대한 미분, h_prev에 대한 미분\n",
    "        # 밑으로 흘려 보내야 밑에 층이(전 시각에서) 학습을 할 테니까. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN으로 언어모델을 만들 것.\n",
    "즉 단어시퀀스에다가 조건부 확률을 구현을 하겠다는...\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
