{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW class\n",
    "* ./ch04/cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit method\n",
    "* ./common/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련할 때 쓰는 method\n",
    "def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "    data_size = len(x)\n",
    "    max_iters = data_size // batch_size\n",
    "    self.eval_interval = eval_interval\n",
    "    model, optimizer = self.model, self.optimizer\n",
    "    total_loss = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(max_epoch):\n",
    "        # 뒤섞기\n",
    "        idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "        x = x[idx]\n",
    "        t = t[idx]\n",
    "\n",
    "        for iters in range(max_iters):\n",
    "            batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "            batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "            # 기울기 구해 매개변수 갱신\n",
    "            loss = model.forward(batch_x, batch_t)\n",
    "            model.backward()\n",
    "            params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "            if max_grad is not None:\n",
    "                clip_grads(grads, max_grad)\n",
    "            optimizer.update(params, grads)\n",
    "            total_loss += loss\n",
    "            loss_count += 1\n",
    "\n",
    "            # 평가\n",
    "            if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                avg_loss = total_loss / loss_count\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                        % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                self.loss_list.append(float(avg_loss))\n",
    "                total_loss, loss_count = 0, 0\n",
    "\n",
    "        self.current_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot method\n",
    "* ./common/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그림 그릴 때 쓰는 method\n",
    "def plot(self, ylim=None):\n",
    "    x = numpy.arange(len(self.loss_list))\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.plot(x, self.loss_list, label='train')\n",
    "    plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "    plt.ylabel('손실')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ./ch04/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ===============================================\n",
    "# config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from ch04.skip_gram import SkipGram\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)     # 입력데이터 context와 라벨 target이 필요\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs                              # word_vecs: 훈련이 끝났을 때 input 벡터들\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}                                              # 공딕셔너리\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ./ch04/eval.py\n",
    "얼마나 단어의 의미를 얼마나 잘 담고있는지 평가 (evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터의 덧셈과 뺄셈으로 **유추문제**를 풀 수 있다. <br>\n",
    "코사인 유사도... 측정 <br>\n",
    "코사인 유사도 가장 높은 것들을 뽑는 것 <br>\n",
    "* 벡터 : 단어에 대응하는 벡터(행)\n",
    "\n",
    "* 의미 뿐만 아니라 문법적인 요소\n",
    "> * 시제\n",
    "> * 복수\n",
    "> * 비교 <br>\n",
    "\n",
    "... 이런 것 까지 담고있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import most_similar, analogy\n",
    "import pickle\n",
    "\n",
    "\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "# pkl_file = 'skipgram_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f: # 파일 불러올거야\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5) # 물어보는 단어(query)를 word_vecs 각 행에 내적...\n",
    "\n",
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs) # children\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs) # worse가 나오길 희망하지만 말뭉치가 작아서 등장빈도가 낮아서 다른 비교형 단어들만 나옴\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analogy 함수\n",
    "* ./common/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None): #top: 상위 n개 출력, answer: 주어진 c와 answer와의 유사도 출력\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort(): # argsort() 오름차순 정렬 후 해당하는 인덱스, -1의 의미: 내림차순\n",
    "        if np.isnan(similarity[i]): # nan이면 pass\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c): # a,b,c들어가면 안되므로 걔네일시 pass\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top: # top의 숫자를 만족하는 순간 끝내\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize 함수\n",
    "* ./common/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ./ch04/skip_gram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.layers import *\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * rn(V, H).astype('f')\n",
    "        W_out = 0.01 * rn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = Embedding(W_in)\n",
    "        self.loss_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "            self.loss_layers.append(layer)\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer] + self.loss_layers\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "\n",
    "        loss = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            loss += layer.forward(h, contexts[:, i])\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            dh += layer.backward(dout)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ./ch04/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ===============================================\n",
    "# config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from ch04.skip_gram import SkipGram\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "# model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "model = SkipGram(vocab_size, hidden_size, window_size, corpus) # 클래스를 skip_gram으로 수정해서 돌리기\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)     # 입력데이터 context와 라벨 target이 필요\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs                              # word_vecs: 훈련이 끝났을 때 input 벡터들\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}                                              # 공딕셔너리\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'skipgram_params.pkl'  # or 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer class\n",
    "* ./common/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy\n",
    "import time # 시간관련 library\n",
    "import matplotlib.pyplot as plt\n",
    "from common.np import *  # import numpy as np\n",
    "from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer): # model: skipgram이나 cbow의 instance, optimizer: adam이나 momentum 클래스의 instance\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None # evaluate interval, 평가를 하겠다는 의미\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20): # 훈련(학습)할 때 쓰는 method\n",
    "        # x: data, t: label,\n",
    "        # 1 epoch:데이터 전부 소진할 때까지 걸리는 시간, 따라서 epoch은 데이터 전부 소진하는 것을 그만큼 반복하겠다는 의미,\n",
    "        # batch_size: 이 만큼씩 묶어서 학습, 한 번 학습할 때 소진하는 data의 개수,\n",
    "        # max_grad: max_gradient 그래디언트 상한선 제한함으로써 기울기 폭발 예방,\n",
    "        # eval_interval: 이만큼 학습할 때마다 평가해서 출력\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size # 1epoch 동안 학습한 횟수\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time() # start 시간 저장\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기(permutation), data가 섞여있지 않은 경우 (정렬돼있는 경우), 학습이 잘 안된다.\n",
    "            # choice보다 개선됨.\n",
    "            # 개선된 점: 랜덤 초이스에 의해 데이터 중복이 돼 어떤 데이터는 학습에 사용이 안될 수도 있는 문제를 permutation으로 해결\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size] # 슬라이싱해서 사용하겠다.\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None): # 그림 그릴 때 쓰는 method\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clip method\n",
    "* ./common/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
